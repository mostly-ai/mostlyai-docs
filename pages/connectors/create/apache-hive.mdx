---
title: 'Use Apache Hive for synthetic data'
description: 'With MOSTLY AI, you can use Apache Hive as a source for original data as well as a destination to deliver synthetic data. To do so, you need to create Apache Hive connectors.'
---

import { Callout } from 'nextra/components';
import { Tabs } from 'nextra/components';
import Image from 'next/image';

# Use Apache Hive for synthetic data

MOSTLY AI can use Apache Hive as a data source for original data as well as a destination for synthetic data. To do so, you need to create Apache Hive connectors.

## Create an Apache Hive connector

For each Apache Hive data source or destination, you need a separate connector.

**Prerequisites**

Obtain the Apache Hive connection details. To use Kerberos, see [Use Kerberos for authentication](#use-kerberos-for-authentication).

- host
- port
- credentials

<Tabs items={['UI', 'Synthetic Data SDK']}>
<Tabs.Tab>
If you use the web application, create a new Apache Hive connector from the **Connectors** page.

**Steps**

1.  From the **Connectors** page, click **New connector**.
    <Image src="/docimages/connectors/create-connector-01-click.webp" alt="Click Create connector button" width={800} height={500} />
2.  From the **Create a new connector** window, select **Apache Hive**.
    <Image src="/docimages/connectors/apache-hive/create-connector-02-select-apache-hive.webp" alt="Select Apache Hive connector" width={800} height={500} />
3.  From the **New connector** page, configure the connector.
    1. For **Name**, enter a name that you can distinguish from other connectors.
    2. For **Access type**, select whether you want to use the connector as a **source** or **destination**.
       <Callout>If you plan to use Apache Hive as a destination for synthetic data, MOSTLY AI recommends that you load the data into Apache Hive using Parquet files. For more information, see [Considerations for Apache Hive as a destination](#considerations-for-apache-hive-as-a-destination).</Callout>
    3. For **Host**, enter the Apache Hive hostname.
    4. For **Port**, enter the port number.
       <Callout type="info">The default port for Apache Hive is 10000.</Callout>
    5. For **Username** and **Password**, enter your Apache Hive credentials.
       <Image src="/docimages/connectors/apache-hive/create-connector-03-configure-apache-hive.webp" alt="Configure Apache Hive connector" width={300} height={500} />
4.  Click **Save** to save your new Apache Hive connector.<br /><br />
    MOSTLY AI tests the connection. If you see an error, check the connection details, update them, and click **Save** again.<br /><br />
    You can click **Save anyway** to save the connector disregarding any errors.

</Tabs.Tab>
<Tabs.Tab>

    If you use the Synthetic Data SDK, create an Apache Hive connector with `mostly.connect()` and provide a connector configuration dictionary as shown below.

```python copy filename="python"
c = mostly.connect({
    "name": "Apache Hive Connector",
    "type": "HIVE",
    "access_type": "SOURCE",
    "config": {
        "host": "hive.hostname.com",
        "port": "10000",
        "username": "mostlyai",
    },
    "secrets": {
        "password": "******"
    },
    "test_connection": True
})
```

</Tabs.Tab>
</Tabs>

## Use Kerberos for authentication

To create an Apache Hive connector with Kerberos authentication, contact your Kerberos system administrator to obtain the information listed below.

- **Kerberos principal**. A unique identity to which Kerberos can assign tickets.
- **Kerberos `krb5.conf` configuration file**. The contents of a `krb5.conf` configuration file, such as the one listed below. For more information, see [`krb5.conf`](https://web.mit.edu/kerberos/krb5-1.12/doc/admin/conf_files/krb5_conf.html) in the _MIT Kerberos Documentation_.

  ```text file=krb5.conf copy
  [libdefaults]
  default_realm = INTERNAL
  dns_lookup_realm = false
  dns_lookup_kdc = false
  forwardable = true
  rdns = false

  [realms]
  INTERNAL = {
  	kdc = ip-172-***-***-222
  	kdc = ip-172-***-***-222.hive-kerberized.domain.com
  	kdc = 172.***.***.222
  	kdc = 3.***.***.111
  	kdc = hive-kerberized.domain.com
  	admin_server = hive-kerberized.domain.com
  }

  [domain_realm]
  .hive-kerberized.domain.com = INTERNAL
  hive-kerberized.domain.com = INTERNAL
  ```

- **Kerberos keytab**. Short for "key table", keytab files are used in Kerberos authentication to store keys needed to log in to Kerberos-aware services. Keytab files allow automated processes (scripts and service authentication) to authenticate using Kerberos without requiring a human to enter a password.
- **CA certificate**. If you have access to your keystore file from which you want to extract the CA certificate, you can use the following command to extract the CA certificate in PEM format.
  ```shell filename="shell" copy
  keytool -exportcert -alias ALIAS -keystore KEYSTORE_FILE -storepass STORE_PASSWORD -file ca-certificate.pem -rfc
  ```
  For example:
  ```shell filename="shell" copy
  keytool -exportcert -alias hivessl-ss -keystore /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/security/hivessl-ss.p12 -storepass 123456 -file /tmp/ca-cert.pem -rfc
  ```

<Tabs items={['UI', 'Synthetic Data SDK']}>
<Tabs.Tab>

If you use the web application, enable the **Authenticate with Kerberos** checkbox in your Apache Hive connector and provide the required information.

**Steps**

1. To use Kerberos for authentication in your Apache Hive connector, select the **Authenticate with Kerberos** checkbox.
2. Provide the Kerberos authentication details.
   1. For **Kerberos principal**, type or paste the Kerberos principal.
   2. For **Kerberos krb5 config**, paste the contents of a `krb5.conf` file.
   3. For **Kerberos keytab**, click the **Upload** button and select a Kerberos keytab file from your local file system.
      <Image src="/docimages/connectors/apache-hive/kerberos/apache-hive-connector-config-with-kerberos.webp" alt="Apache Hive connector configuration - Kerberos authentication" width={800} height={500} />
3. Enable **SSL** and provide the CA certificate.
   <Image src="/docimages/connectors/apache-hive/kerberos/apache-hive-connector-config-with-kerberos-enable-ssl.webp" alt="Apache Hive connector configuration - Kerberos authentication with SSL" width={400} height={150} />
4. Click **Save**.

</Tabs.Tab>
<Tabs.Tab>

If you use the Synthetic Data SDK, use the configuration as listed below.

```python filename="python" copy
# Import required Python libraries and initialize MostlyAI() client
import pandas as pd
import base64
from mostlyai.sdk import MostlyAI
mostly = MostlyAI(api_key=INSERT_API_KEY)

# Read the contents of your Kerberos keytab file and encode them to a base64 string

keytab_data = open('kerberos/hive.keytab', 'rb').read()
base64_keytab_string = base64.b64encode(keytab_data).decode('utf-8')

# Read the contents of your CA certificate PEM file. It should already be base64-encoded

base64_ssl_cert_string = open('kerberos/ca_cert.pem', 'rb').read()

# Create a multi-line string with the contents of your krb5.conf file

# Note: Contains examples of configuration values some of which are masked with asterisks (\*)

kerberos_krb5_conf = """
[libdefaults]
default_realm = INTERNAL
dns_lookup_realm = false
dns_lookup_kdc = false
forwardable = true
rdns = false

[realms]
INTERNAL = {
kdc = ip-172-**_-_**-222
kdc = ip-172-**_-_**-222.hive-kerberized.domain.com
kdc = 172.**_._**.222
kdc = 3.75.170.111
kdc = hive-kerberized.domain.com
admin_server = hive-kerberized.domain.com
}

[domain_realm]
.hive-kerberized.domain.com = INTERNAL
hive-kerberized.domain.com = INTERNAL
"""

# Create a `config` variable with the contents of your Apache Hive connector configuration

config = {
"name": "Apache Hive Connector with Kerberos auth",
"type": "HIVE",
"access_type": "SOURCE",
"config": {
"host": "hive-kerberized.test.mostlylab.com",
"port": 10000,
"kerberos_enabled": True,
"kerberos_principal": "hive/hive-kerberized.test.mostlylab.com@INTERNAL",
"kerberos_krb5_conf": kerberos_krb5_conf,
"ssl_enabled": True
},
"secrets": {
"kerberos_keytab": base64_keytab_string
},
"ssl": {
"ca_certificate": base64_ssl_cert_string
},
"test_connection": True
}

# Create the connector

c = mostly.connect(config=config)
```

</Tabs.Tab>
</Tabs>

**What's next**

Depending on whether you created a **source** or a **destination** connector, you can use the connector as:

- [data source](../use/data-source) for a new generator
- [data destination](../use/data-destination) for a new synthetic dataset

## Considerations for Apache Hive as a destination

The Apache Hive connector uses a database interface to connect to your Apache Hive instance. While the database interface provides high-performance for read operations (for example, to pull original data for synthesis), using that interface to load synthetic data in batches can be too slow. It is not a recommended solution for delivering high volumes of synthetic data.

The fastest way to load data into Apache Hive is to save synthetic data as Parquet files into an S3 storage bucket and then load the data into Apache Hive.

For example:

```sql filename="hive"
CREATE TABLE table_name;
LOAD DATA INPATH 's3://bucket-name/path/to/parquet-file' INTO TABLE table_name;
```
